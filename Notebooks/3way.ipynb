{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('Cleaned-P&P data.csv')\n",
    "\n",
    "blanks = []  # start with an empty list`\n",
    "\n",
    "for i,c,d,inc,sd in df1.itertuples():  # iterate over the DataFrame\n",
    "        if c == 'O' or c=='E':         # test 'review' for whitespace\n",
    "            blanks.append(i)     # add matching index numbers to the list\n",
    "df1.drop(blanks, inplace=True)\n",
    "df1[\"desc\"] = df1[\"short_des\"] + '. ' + df1[\"desc\"]\n",
    "df1.reset_index(drop=True,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2 = pd.read_csv('Cleaned-P&P data.csv')\n",
    "blanks2 = []  # start with an empty list`\n",
    "\n",
    "for i,c,d,inc,sd in df2.itertuples():  # iterate over the DataFrame\n",
    "        if c == 'O' or c=='E':         # test 'review' for whitespace\n",
    "            blanks2.append(i)     # add matching index numbers to the list\n",
    "df2.drop(blanks2, inplace=True)\n",
    "df2.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the texts\n",
    "import re\n",
    "corpus = []\n",
    "all_words = []\n",
    "max_len=0\n",
    "for i in range(0, 490):\n",
    "\n",
    "    review = re.sub('\\w\\d{7}', ' ', dataset['desc'][i])\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = ' '.join(review)\n",
    "    \n",
    "    review = nlp(review)\n",
    "    review = [word.text for word in review]\n",
    "   # for word.text in review:\n",
    "        \n",
    "    all_words.append(review) \n",
    "    if len(review) > max_len:\n",
    "        max_len = len(review)\n",
    "    ds = ' '.join(review)\n",
    "#    for word2vec we want an array of vectors\n",
    "    corpus.append(ds)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardise key terms in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpf = pd.read_csv('Del-Pass.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     delivery subscription service\n",
       "1        delivery pass subscription\n",
       "2             delivery pass service\n",
       "3                free delivery pass\n",
       "4                   delivery passes\n",
       "5                    delivery pass \n",
       "6        p&p delivery subscription \n",
       "7            delivery subscriptions\n",
       "8             delivery subscription\n",
       "9                      delivery sub\n",
       "10                      p&p service\n",
       "11                              p&p\n",
       "12                     subscription\n",
       "Name: kpattern, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpf['kpattern']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_list = dpf['kpattern'].tolist()\n",
    "regex = re.compile(r'(' + '|'.join(keys_list) + r')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_list = ['remove','removed','cancelled', 'cancel' ,'deleted']\n",
    "regey = re.compile(r'(' + '|'.join(cus_list) + r')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "additional =[\"thank you\",\"please\",\"hello\",\"hi\",\"advise\",\"hin't\",\"st\",\"nd\",\"rd\",\"th\",\"thank\",\"pnt\",\"work request\"]\n",
    "stopwords = stopwords + additional\n",
    "remov=[]\n",
    "for ele in stopwords:\n",
    "    matches = re.findall(\"n't\",ele)\n",
    "    matches2 = re.findall(\"'nt\",ele)\n",
    "    if len(matches)>0 or len(matches2)>0:\n",
    "        remov.append(ele)\n",
    "stopwords = [word for word in stopwords if word not in remov]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cldict = {'P':1 , 'R':0}\n",
    "dataset['Class Label'] = dataset['class'].map(cldict)\n",
    "label_series = pd.Series(dataset['Class Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(490,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "class_label = np.array(label_series)\n",
    "y = class_label\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>desc</th>\n",
       "      <th>inc_num</th>\n",
       "      <th>short_des</th>\n",
       "      <th>Class Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P</td>\n",
       "      <td>y2684992. good afternoon,\\r\\n\\r\\nthis customer...</td>\n",
       "      <td>INC0809260</td>\n",
       "      <td>y2684992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R</td>\n",
       "      <td>delivery pass needs removing on staff accounts...</td>\n",
       "      <td>INC0808628</td>\n",
       "      <td>delivery pass needs removing on staff accounts...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R</td>\n",
       "      <td>delivery pass needs removing on staff accounts...</td>\n",
       "      <td>INC0807253</td>\n",
       "      <td>delivery pass needs removing on staff accounts...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P</td>\n",
       "      <td>customer being charged for delivery. customer:...</td>\n",
       "      <td>INC0806836</td>\n",
       "      <td>customer being charged for delivery</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P</td>\n",
       "      <td>website/web shop issues. short description:tam...</td>\n",
       "      <td>INC0805994</td>\n",
       "      <td>website/web shop issues</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                               desc     inc_num  \\\n",
       "0     P  y2684992. good afternoon,\\r\\n\\r\\nthis customer...  INC0809260   \n",
       "1     R  delivery pass needs removing on staff accounts...  INC0808628   \n",
       "2     R  delivery pass needs removing on staff accounts...  INC0807253   \n",
       "3     P  customer being charged for delivery. customer:...  INC0806836   \n",
       "4     P  website/web shop issues. short description:tam...  INC0805994   \n",
       "\n",
       "                                           short_des  Class Label  \n",
       "0                                           y2684992            1  \n",
       "1  delivery pass needs removing on staff accounts...            0  \n",
       "2  delivery pass needs removing on staff accounts...            0  \n",
       "3                customer being charged for delivery            1  \n",
       "4                            website/web shop issues            1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=47)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segment_featurizer = SegmentFeaturizer()  # more on this below\n",
    "class CustomLinguisticFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass   \n",
    "  #  def fit(self, X, y=None):\n",
    "    def fit(self, X, y):\n",
    "        return self    \n",
    "    def transform(self, X):       \n",
    "        ref_corpus=[]\n",
    "        for text in X:           \n",
    "            trans = regex.sub(lambda m: m.group().replace(m.group(),\"del-pass\"),text)\n",
    "            trans_ref = regey.sub(lambda n: n.group().replace(n.group(),\"delete\"),trans)\n",
    "             \n",
    "            ref_corpus.append(trans_ref)\n",
    "       \n",
    "        return ref_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Vector Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class SpacyVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        self.dim = 300\n",
    "        pass\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Doc.vector defaults to an average of the token vectors.\n",
    "        # https://spacy.io/api/doc#vector\n",
    "        lemmatized_list=[]\n",
    "        for snt in X:\n",
    "        #    print(snt)\n",
    "            tokens = nlp(snt)\n",
    "        #    for token in tokenized:\n",
    "            filtered_sentence = [w.text for w in tokens if not w.text in stopwords]\n",
    "        #                 lemm = token.lemma_ for token.text in token\n",
    "        #             lemmatized_list.append(lemm)\n",
    "            stri = ' '.join(filtered_sentence)\n",
    "            lemmatized_list.append(stri)\n",
    "        return [self.nlp(text).vector for text in lemmatized_list]\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "embeddings_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"mean_embeddings\", SpacyVectorTransformer(nlp)),\n",
    "        (\"reduce_dim\", TruncatedSVD(50)),\n",
    "        (\"classifier\", RandomForestClassifier()),\n",
    "    ]\n",
    ")\n",
    "embeddings_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "embeddings_pipeline2 = Pipeline(\n",
    "    steps=[\n",
    "        (\"mean_embeddings\", SpacyVectorTransformer(nlp)),\n",
    "        (\"reduce_dim\", TruncatedSVD(50)),\n",
    "        (\"classifier\", LinearSVC()),\n",
    "    ]\n",
    ")\n",
    "embeddings_pipeline2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "svm = LinearSVC()\n",
    "embeddings_pipeline3 = Pipeline(\n",
    "    steps=[\n",
    "        (\"mean_embeddings\", SpacyVectorTransformer(nlp)),\n",
    "        (\"reduce_dim\", TruncatedSVD(50)),\n",
    "        (\"classifier\", CalibratedClassifierCV(svm)),\n",
    "    ]\n",
    ")\n",
    "embeddings_pipeline3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "svm = LinearSVC()\n",
    "embeddings_pipeline4 = Pipeline(\n",
    "    steps=[\n",
    "        (\"cust\",CustomLinguisticFeatureTransformer()),\n",
    "#        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"mean_embeddings\", SpacyVectorTransformer(nlp)),\n",
    "        (\"reduce_dim\", TruncatedSVD(200)),\n",
    "#         (\"dict_vect\", DictVectorizer()),\n",
    "        (\"classifier\", CalibratedClassifierCV(svm)),\n",
    "    ]\n",
    ")\n",
    "embeddings_pipeline4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#svm = LinearSVC()\n",
    "embeddings_pipeline5 = Pipeline(\n",
    "    steps=[\n",
    "        (\"cust\",CustomLinguisticFeatureTransformer()),\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "#        (\"mean_embeddings\", SpacyVectorTransformer(nlp)),\n",
    "#        (\"reduce_dim\", TruncatedSVD(200)),\n",
    "#         (\"dict_vect\", DictVectorizer()),\n",
    "#        (\"rdf\", RandomForestClassifier()),\n",
    "        (\"mnb\",MultinomialNB()),\n",
    "    ]\n",
    ")\n",
    "embeddings_pipeline5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = embeddings_pipeline4.predict(X_test)\n",
    "cr = classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list1 = ['issue with delivery subscriptions Piara called in and reported an issue that one of her customer purchased delivery subscriptions in month of january for year costs pounds but still got charged for delivery kindly assist ']\n",
    "#list1 =['delivery subscription not active on account please can you activate delivery subscription rv (purchased ) on account thanks ']\n",
    "#list1 = ['delivery pass needs removing on staff accounts orders supra dba req please can the delivery pass be removed from the attached staff accounts thanks ']\n",
    "list1 = ['application session restarted before raising incidents customer unable to edit the address for the same application session restarted before raising incidents customer also unable to edit the address for the same']\n",
    "print(embeddings_pipeline4.predict(list1))\n",
    "probs = embeddings_pipeline4.predict_proba(list1)\n",
    "np.around(probs, decimals = 3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(embeddings_pipeline5, X, y, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_mean_score = np.mean(cross_val_score(embeddings_pipeline3, X, y, cv = 10))\n",
    "clf_score = embeddings_pipeline3.score(X_test, y_test)\n",
    "cv_mean_score, clf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = embeddings_pipeline5.predict(X)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_score = embeddings_pipeline3.score(X, y)\n",
    "clf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Faulty_Xtest =[]\n",
    "for i in range(0,490):\n",
    "    if y[i] != predictions[i]:\n",
    "        print(f'predicted class of:{X[i]} is :{predictions[i]}', f'Index is : {i}')\n",
    "        Faulty_Xtest.append(X[i])\n",
    "arr = np.array(Faulty_Xtest)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['inc_num'][201], dataset['desc'][201], dataset['class'][201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here:\n",
    "\n",
    "for token in nlp(X_test[94]):\n",
    "    print(f'{token.text:{12}} {token.pos_:{6}} {token.dep_:{6}} {spacy.explain(token.dep_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from spacy import displacy\n",
    "displacy.render(nlp(X_test[95]), style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for match in re.finditer(\"delivery subscription \",X_test[94]):\n",
    "    X_test[94]\n",
    "    print(match.span())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_list=[]\n",
    "for snt in X[:5]:\n",
    "    print(snt)\n",
    "    tokens = nlp(snt)\n",
    "#    for token in tokenized:\n",
    "    filtered_sentence = [w.text for w in tokens if not w.text in stopwords]\n",
    "#                 lemm = token.lemma_ for token.text in token\n",
    "#             lemmatized_list.append(lemm)\n",
    "    stri = ' '.join(filtered_sentence)\n",
    "    lemmatized_list.append(stri)\n",
    "lemmatized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matche = re.findall(\"delivery subscription \",X_test[94])\n",
    "match.span()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile('([a-zA-Z]\\\"[a-zA-Z])', re.S)\n",
    "myfile =  'foo\"s bar'\n",
    "myfile2 = regex.sub(lambda m: m.group().replace('\"',\"%\",1), myfile)\n",
    "print(myfile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lst = ['delivery subscription service', 'delivery pass subscription','delivery pass service','delivery passes','delivery pass ','p&p delivery subscription ','delivery subscriptions','delivery subscription']\n",
    "\n",
    "def replace_double_quote(match):\n",
    "    text = match.group()\n",
    "    print(match.group())\n",
    "    return text.replace(text, 'del-pass')\n",
    "regex = re.compile(r'(' + '|'.join(lst) + r')')\n",
    "myfile = 'please can you apply the annual delivery subscription is activated on this customers account purchased still being charged for delivery thank you'\n",
    "regex.sub(replace_double_quote, myfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lst = ['foo', 'bar', 'and']\n",
    "def my_func():\n",
    "    \n",
    "    regex = re.compile(r'(' + '|'.join(lst) + r')')\n",
    "    myfile = 'foo\"s bar and bar\"s foo'\n",
    "   \n",
    "    pxk = regex.sub(lambda m: m.group().replace(m.group(),\"rap\",1), myfile)\n",
    "    return pxk\n",
    "#     def replace_double_quote(match):\n",
    "#         text = match.group()\n",
    "#         print(text)\n",
    "#         return text.replace(text, 'rap')\n",
    "   \n",
    "print(my_func())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def match_kpatterns(match):\n",
    "    \n",
    "    text = match.group()\n",
    "    print(match.group())\n",
    "    return text.replace(text, 'del-pass')\n",
    "\n",
    "\n",
    "regez = re.compile(r'(' + '|'.join(keys_list) + r')')\n",
    "seg = \"please can you apply the annual delivery subscription is activated on this customers account purchased still being charged for delivery thank you\"\n",
    "\n",
    "regez.sub(match_kpatterns, seg)\n",
    "#    return regex.sub(lambda m: m.group().replace(m.group(),\"del-pass\"), rfseg)\n",
    "#r_featurize(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nlp(u'trashbin')\n",
    "text1 = nlp(u'dustbin')\n",
    "fr = text1.similarity(text)\n",
    "fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,488):\n",
    "    matches = re.findall(\"n t \",corpus[i])\n",
    "    print(f'mathc is : {matches}', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm=[]\n",
    "#for snt in corpus[0]:\n",
    "kr = nlp(corpus[0])\n",
    "for token in kr:\n",
    "    ls = token.lemma_\n",
    "    lemm.append(ls)\n",
    "lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp(lemm[0]).vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from joblib import dump, load\n",
    "dump(embeddings_pipeline4, filename = 'ii' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf_model = NMF(n_components=3,random_state=42)\n",
    "# This can take awhile, we're dealing with a large amount of documents!\n",
    "nmf_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "loaded_model2 = load(filename = 'P&P Classifier V_0.1.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = loaded_model2.predict(X)\n",
    "cr = classification_report(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       190\n",
      "           1       1.00      1.00      1.00       300\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       490\n",
      "   macro avg       1.00      1.00      1.00       490\n",
      "weighted avg       1.00      1.00      1.00       490\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[190,   0],\n",
       "       [  0, 300]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cr)\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "confusion_matrix(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
