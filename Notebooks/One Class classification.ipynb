{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 9990, 1: 10})\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate and plot a synthetic imbalanced classification dataset\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    "\tn_clusters_per_class=1, weights=[0.999], flip_y=0, random_state=4)\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "\trow_ix = where(y == label)[0]\n",
    "\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using One-Class SVM\n",
    "https://machinelearningmastery.com/one-class-classification-algorithms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define outlier detection model\n",
    "from sklearn.svm import OneClassSVM\n",
    "model = OneClassSVM(gamma='scale', nu=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma='scale', kernel='rbf',\n",
       "      max_iter=-1, nu=0.01, random_state=None, shrinking=True, tol=0.001,\n",
       "      verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit on majority class\n",
    "trainX = trainX[trainy==0]\n",
    "model.fit(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect outliers in the test set\n",
    "yhat = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2000):\n",
    "    if yhat[i] == 0:\n",
    "        print(i, yhat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "834 1\n",
      "2996 1\n",
      "3965 1\n",
      "6111 1\n",
      "7318 1\n",
      "7832 1\n",
      "8436 1\n",
      "8505 1\n",
      "8559 1\n",
      "9228 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10000):\n",
    "    if y[i] == 1:\n",
    "        print(i, y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp_course\\lib\\site-packages\\IPython\\core\\async_helpers.py\u001b[0m in \u001b[0;36m_pseudo_sync_runner\u001b[1;34m(coro)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \"\"\"\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mcoro\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp_course\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_async\u001b[1;34m(self, raw_cell, store_history, silent, shell_futures)\u001b[0m\n\u001b[0;32m   2986\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstore_history\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2987\u001b[0m             self.history_manager.store_inputs(self.execution_count,\n\u001b[1;32m-> 2988\u001b[1;33m                                               cell, raw_cell)\n\u001b[0m\u001b[0;32m   2989\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msilent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2990\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_cell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp_course\\lib\\site-packages\\IPython\\core\\history.py\u001b[0m in \u001b[0;36mstore_inputs\u001b[1;34m(self, line_num, source, source_raw)\u001b[0m\n\u001b[0;32m    717\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_hist_raw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_raw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdb_input_cache_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdb_input_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_raw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m             \u001b[1;31m# Trigger to flush cache and write to DB.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# mark inliers 1, outliers -1\n",
    "testy[testy == 1] = -1\n",
    "testy[testy == 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "cr = classification_report(testy, yhat)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-class svm for imbalanced binary classification\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import OneClassSVM\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    "\tn_clusters_per_class=1, weights=[0.999], flip_y=0, random_state=4)\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2, stratify=y)\n",
    "# define outlier detection model\n",
    "model = OneClassSVM(gamma='scale', nu=0.01)\n",
    "# fit on majority class\n",
    "trainX = trainX[trainy==0]\n",
    "model.fit(trainX)\n",
    "# detect outliers in the test set\n",
    "yhat = model.predict(testX)\n",
    "# mark inliers 1, outliers -1\n",
    "testy[testy == 1] = -1\n",
    "testy[testy == 0] = 1\n",
    "# calculate score\n",
    "score = f1_score(testy, yhat, pos_label=-1)\n",
    "\n",
    "print('F1 Score: %.3f' % score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score2 = f1_score(testy, yhat, pos_label=1)\n",
    "print('F1 Score: %.3f' % score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('Cleaned-P&P data.csv')\n",
    "\n",
    "blanks = []  # start with an empty list`\n",
    "\n",
    "# for i,c,d,inc,sd in df1.itertuples():  # iterate over the DataFrame\n",
    "#         if c == 'O' or c=='E':         # test 'review' for whitespace\n",
    "#             blanks.append(i)     # add matching index numbers to the list\n",
    "df1.drop(blanks, inplace=True)\n",
    "df1[\"desc\"] = df1[\"short_des\"] + '. ' + df1[\"desc\"]\n",
    "df1.reset_index(drop=True,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2 = pd.read_csv('Cleaned-P&P data.csv')\n",
    "blanks2 = []  # start with an empty list`\n",
    "\n",
    "# for i,c,d,inc,sd in df2.itertuples():  # iterate over the DataFrame\n",
    "#         if c == 'O' or c=='E':         # test 'review' for whitespace\n",
    "#             blanks2.append(i)     # add matching index numbers to the list\n",
    "df2.drop(blanks2, inplace=True)\n",
    "df2.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the texts\n",
    "import re\n",
    "corpus = []\n",
    "all_words = []\n",
    "max_len=0\n",
    "for i in range(0, 504):\n",
    "\n",
    "    review = re.sub('\\w\\d{7}', ' ', dataset['desc'][i])\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = ' '.join(review)\n",
    "    \n",
    "    review = nlp(review)\n",
    "    review = [word.text for word in review]\n",
    "   # for word.text in review:\n",
    "        \n",
    "    all_words.append(review) \n",
    "    if len(review) > max_len:\n",
    "        max_len = len(review)\n",
    "    ds = ' '.join(review)\n",
    "#    for word2vec we want an array of vectors\n",
    "    corpus.append(ds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpf = pd.read_csv('Del-Pass.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_list = dpf['kpattern'].tolist()\n",
    "regex = re.compile(r'(' + '|'.join(keys_list) + r')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_list = ['remove','removed','cancelled', 'cancel' ,'deleted']\n",
    "regey = re.compile(r'(' + '|'.join(cus_list) + r')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "additional =[\"thank you\",\"please\",\"hello\",\"hi\",\"advise\",\"hin't\",\"st\",\"nd\",\"rd\",\"th\",\"thank\"]\n",
    "stopwords = stopwords + additional\n",
    "remov=[]\n",
    "for ele in stopwords:\n",
    "    matches = re.findall(\"n't\",ele)\n",
    "    matches2 = re.findall(\"'nt\",ele)\n",
    "    if len(matches)>0 or len(matches2)>0:\n",
    "        remov.append(ele)\n",
    "stopwords = [word for word in stopwords if word not in remov]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cldict = {'P':1 , 'R':1, 'E':-1,'O':-1}\n",
    "dataset['Class Label'] = dataset['class'].map(cldict)\n",
    "label_series = pd.Series(dataset['Class Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "class_label = np.array(label_series)\n",
    "y = class_label\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=47)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class SpacyVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        self.dim = 300\n",
    "        pass\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Doc.vector defaults to an average of the token vectors.\n",
    "        # https://spacy.io/api/doc#vector\n",
    "        lemmatized_list=[]\n",
    "        for snt in X:\n",
    "        #    print(snt)\n",
    "            tokens = nlp(snt)\n",
    "        #    for token in tokenized:\n",
    "            filtered_sentence = [w.text for w in tokens if not w.text in stopwords]\n",
    "        #                 lemm = token.lemma_ for token.text in token\n",
    "        #             lemmatized_list.append(lemm)\n",
    "            stri = ' '.join(filtered_sentence)\n",
    "            lemmatized_list.append(stri)\n",
    "        return [self.nlp(text).vector for text in lemmatized_list]\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segment_featurizer = SegmentFeaturizer()  # more on this below\n",
    "class CustomLinguisticFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass   \n",
    "  #  def fit(self, X, y=None):\n",
    "    def fit(self, X, y):\n",
    "        return self    \n",
    "    def transform(self, X):       \n",
    "        ref_corpus=[]\n",
    "        for text in X:           \n",
    "            trans = regex.sub(lambda m: m.group().replace(m.group(),\"del-pass\"),text)\n",
    "            trans_ref = regey.sub(lambda n: n.group().replace(n.group(),\"delete\"),trans)\n",
    "             \n",
    "            ref_corpus.append(trans_ref)\n",
    "       \n",
    "        return ref_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#segment_featurizer = SegmentFeaturizer()  # more on this below\n",
    "class Denseconverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass   \n",
    "  #  def fit(self, X, y=None):\n",
    "    def fit(self, X, y):\n",
    "        return self    \n",
    "    def transform(self, X):       \n",
    "        \n",
    "       \n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = []\n",
    "for i in range(0,403):\n",
    "    if y_train[i] == -1:\n",
    "  #      y_train.pop[i]\n",
    "         ind.append(i)\n",
    "X_train = np.array(X_train)\n",
    "trainy = np.delete(y_train, ind)\n",
    "trainx = np.delete(X_train, ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx=list(trainx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "embeddings_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"cust\",CustomLinguisticFeatureTransformer()),\n",
    "        (\"mean_embeddings\", SpacyVectorTransformer(nlp)),\n",
    "        (\"reduce_dim\", TruncatedSVD(50)),\n",
    "        (\"classifier\", OneClassSVM(gamma='scale', nu=0.03)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "embeddings_pipeline.fit(trainx,trainy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_pipeline2 = OneClassSVM(gamma='scale', nu=0.01)\n",
    "train_X = [nlp(str(text)).vector for text in trainx]\n",
    "embeddings_pipeline2.fit(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elyptic Envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "#train_X = [nlp(str(text)).vector for text in trainx]\n",
    "#embeddings_pipeline3 = EllipticEnvelope(contamination=0.01)\n",
    "embeddings_pipeline3 = Pipeline(\n",
    "    steps=[\n",
    "        (\"cust\",CustomLinguisticFeatureTransformer()),\n",
    "        (\"mean_embeddings\", SpacyVectorTransformer(nlp)),\n",
    "#        (\"reduce_dim\", TruncatedSVD(50)),\n",
    "        (\"EllipticEnvelope\", EllipticEnvelope(contamination=0.01)),\n",
    "    ]\n",
    ")\n",
    "embeddings_pipeline3.fit(trainx,trainy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from numpy import vstack\n",
    "# test_X = [nlp(text).vector for text in X_test]\n",
    "# train_X = [nlp(str(text)).vector for text in trainx]\n",
    "def lof_predict(embeddings_pipeline4, X_train, X_test):\n",
    "    # create one large dataset\n",
    "    composite = X_train + X_test\n",
    "    #composite = vstack((X_train, X_test))\n",
    "    #generalize\n",
    "    ref_corpus=[]\n",
    "    for text in composite:           \n",
    "            trans = regex.sub(lambda m: m.group().replace(m.group(),\"del-pass\"),text)\n",
    "            trans_ref = regey.sub(lambda n: n.group().replace(n.group(),\"delete\"),trans)\n",
    "             \n",
    "            ref_corpus.append(trans_ref)\n",
    "\n",
    "    #clean data\n",
    "    lemmatized_list=[]\n",
    "    for snt in ref_corpus:\n",
    "        #    print(snt)\n",
    "            tokens = nlp(snt)\n",
    "        #    for token in tokenized:\n",
    "            filtered_sentence = [w.text for w in tokens if not w.text in stopwords]\n",
    "        #                 lemm = token.lemma_ for token.text in token\n",
    "        #             lemmatized_list.append(lemm)\n",
    "            stri = ' '.join(filtered_sentence)\n",
    "            lemmatized_list.append(stri)\n",
    "    lemmatized_list2 = [nlp(text).vector for text in lemmatized_list]\n",
    "    \n",
    "    # make prediction on composite dataset\n",
    "    yhat = embeddings_pipeline4.fit_predict(lemmatized_list2)\n",
    "    \n",
    "    # return just the predictions on the test set\n",
    "    return yhat[len(X_train):]\n",
    "embeddings_pipeline4 = LocalOutlierFactor(contamination=0.036)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls = [nlp('g7121564 please set ACUSTRSS  to so account holder can receive statements ').vector]\n",
    "# test_X.append(ls)\n",
    "yhat = lof_predict(embeddings_pipeline4, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stri='laptop america straw eat glucose'\n",
    "abc=[]\n",
    "abc.append(stri)\n",
    "print(len(abc[0]))\n",
    "rat = lof_predict(embeddings_pipeline4, X_train, abc)\n",
    "rat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "lof_cr = classification_report(y_test, yhat)\n",
    "print(lof_cr)\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "print(confusion_matrix(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Faulty_Xtest =[]\n",
    "tot = 0\n",
    "for i in range(0,102):\n",
    "    if yhat[i] != y_test[i]:\n",
    "        tot+=1\n",
    "        print(f'Text:{X_test[i]}||', f'predicted class:{yhat[i]}||', f'Index is : {i}')\n",
    "        Faulty_Xtest.append(X_test[i])\n",
    "print(f'total is: {tot}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tfidf with oneclass SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "embeddings_pipeline5 = Pipeline(\n",
    "    steps=[\n",
    "        (\"cust\",CustomLinguisticFeatureTransformer()),\n",
    "#        (\"mean_embeddings\", SpacyVectorTransformer(nlp)),\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "#        (\"reduce_dim\", TruncatedSVD(50)),\n",
    "        (\"classifier\", OneClassSVM(gamma='scale', nu=0.01)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "embeddings_pipeline5.fit(trainx,trainy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tfid with elyptic envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#train_X = [nlp(str(text)).vector for text in trainx]\n",
    "nparray = np.array(trainx)\n",
    "#embeddings_pipeline3 = EllipticEnvelope(contamination=0.01)\n",
    "embeddings_pipeline6 = Pipeline(\n",
    "    steps=[\n",
    "        (\"cust\",CustomLinguisticFeatureTransformer()),\n",
    "        ('tfidf', TfidfVectorizer()),        \n",
    "        (\"reduce_dim\", TruncatedSVD(100)),\n",
    "#        (\"Denseconverter\",Denseconverter()),\n",
    "        (\"EllipticEnvelope\", EllipticEnvelope(contamination=0.01)),\n",
    "    ]\n",
    ")\n",
    "embeddings_pipeline6.fit(nparray,trainy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#test_X = [nlp(str(text)).vector for text in X_test]\n",
    "#test_X = [nlp('g7121564 please set ACUSTRSS  to so account holder can receive statements ').vector]\n",
    "y_pred = embeddings_pipeline5.predict(X_test)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = ['customer purchased delivery aubscription rv fm she is still being charged deliver please can you activate delivery subscription thank you']\n",
    "#list1 = ['g7121564 please set ACUSTRSS  to so account holder can receive statements']\n",
    "#list1 = [nlp('g7121564 please set ACUSTRSS  to so account holder can receive statements ').vector]\n",
    "#list1 = np.array(list1)\n",
    "print(embeddings_pipeline.predict(list1))\n",
    "#probs = embeddings_pipeline.predict_proba(list1)\n",
    "#np.around(probs, decimals = 3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,101):\n",
    "    if y_test[i] == -1:\n",
    "        print(X_test[i], y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Faulty_Xtest =[]\n",
    "tot = 0\n",
    "for i in range(0,101):\n",
    "    if y_pred[i] != y_test[i]:\n",
    "        tot+=1\n",
    "        print(f'Text:{X_test[i]}||', f'predicted class:{y_pred[i]}||', f'Index is : {i}')\n",
    "        Faulty_Xtest.append(X_test[i])\n",
    "print(f'total is: {tot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(Faulty_Xtest)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'OneClassSVM.pk1'\n",
    "pickle.dump(embeddings_pipeline, open(filename, 'wb'))\n",
    "#embeddings_pipeline.write().overwrite().save('OneClassSVM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(embeddings_pipeline4, filename = 'LocalOutlierFac.joblib' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load(filename = 'LocalOutlierFac.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = [nlp(str(text)).vector for text in X_test]\n",
    "\n",
    "joblib_preds = loaded_model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "confusion_matrix(y_test,joblib_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = lof_predict(loaded_model, trainx, test_X)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "confusion_matrix(y_test,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('tranx',trainx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = cv.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "LDA = LatentDirichletAllocation(n_components=2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can take awhile, we're dealing with a large amount of documents!\n",
    "LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,topic in enumerate(LDA.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
